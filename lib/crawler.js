/*!
 * Crawl - crawler
 * Copyright(c) 2012 Mike Moulton <mike@meltmedia.com>
 * MIT Licensed
 */

var events = require("events"),
    Crawler = require('simplecrawler').Crawler,
    hash = require('node_hash'),
    _ = require('underscore'),
    fs = require("fs"),
    urlUtil = require('url');

var crawler = new events.EventEmitter();

/**
 * crawler.crawl
 *
 * Crawls a website, starting at `url`, finding all linked pages within the same domain.
 * The `url` can also be a filesystem path containing a stringified JSON object of a past crawl. This
 * can be generated using the included CLI and the '--json' option.
 *
 * This function also supports several `options`:
 *   - headers {Boolean}: include the raw http headers from the response in the results
 *   - body {Boolean}: include the http response body in results
 *
 * This function is asyncronous and requires a `callback` with the following signature:
 *   - function(err, pages)
 * where `pages` is an array with the following object structure for each result:
 *   {
 *     url: URL Object provided from Node URL parser,
 *     status: HTTP status code
 *     contentType: the MIME type for the resource
 *     checksum: SHA 256 hash of the response body,
 *     links: Array of links found on this page,
 *     body: The response body (optional),
 *     headers: The response headers (optional),
 *     date: Timestamp of the page crawl
 *   }
 *
 * @param {String} http(s) url of site to crawl, or filesystem path to JSON file to import
 * @param {Object} options (optional)
 * @param {Function} callback -- function(err, pages)
 * @api public
 */
crawler.crawl = function(url, options, callback) {
  if (typeof options === 'function') {
    callback = options;
    options = {};
  }
  options = options || {};

  // setup some default options for the node.io job
  options['crawler'] = crawler;

	var pages = {},
      urlParts = urlUtil.parse(url, true);

  // do a web crawl of url if it's an http protocol
	if (urlParts.protocol == "https:" || urlParts.protocol == "http:") {

    var port = urlParts.port ? urlParts.port : 80,
        siteCrawler = new Crawler(urlParts.hostname, urlParts.path, port);

    // configure crawler
    siteCrawler.interval = 10;
    siteCrawler.maxConcurrency = 10;
    siteCrawler.scanSubdomains = true;
    siteCrawler.downloadUnsupported = false;

    if (options.username && options.password) {
      siteCrawler.needsAuth = true;
      siteCrawler.authUser = options.username;
      siteCrawler.authPass = options.password;
    }

    function mimeTypeSupported(MIMEType) {
      var supported = false;
      siteCrawler.supportedMimeTypes.forEach(function(mimeCheck) {
        if (!!mimeCheck.exec(MIMEType)) {
          supported = true;
        }
      });
      return supported;
    }

    var pageHandler = function(queueItem, responseBuffer, response) {
      if (mimeTypeSupported(queueItem.stateData.contentType)) {
        crawler.emit("crawl", queueItem.url);

        var data = responseBuffer.toString(),
            page = {
              url: queueItem.url,
              status: queueItem.stateData.code,
              contentType: queueItem.stateData.contentType,
              checksum: hash.sha256(data),
              date: new Date().toJSON()
            };
        if (options.headers) page.headers = queueItem.stateData.headers;
        if (options.body) page.body = data;

        pages[queueItem.url] = page;
      }
    };

    siteCrawler.on("discoverycomplete", function(queueItem, resources) {
      pages[queueItem.url].links = resources;
    });

    // handle normal pages
    siteCrawler.on("fetchcomplete", pageHandler);

    // handle broken links
    siteCrawler.on("fetch404", pageHandler);

    // on completion, parse broken links and report
    siteCrawler.on("complete", function(queueItem, responseBuffer, response) {
      callback(null, _.map(pages, function (value, key) { return value; }) );
    });

    siteCrawler.start(); // crawl the site

	}

  // otherwise we load a json file, assumed to be generated by the CLI using the '--json' option
  else if (urlParts.path) {
    var path = urlParts.path;
    try {
      if (fs.statSync(path).isFile()) {
        fs.readFile(path, function(err, data) {
          if (err) callback(err);

          var pages = JSON.parse(data);
          callback(null, pages);
        });
      }
    }
    catch (err) {
      callback(err);
    }
  }

  else {
    callback("Unable to interperate url as path or web address: %s", url);
  }

};

module.exports = crawler;
